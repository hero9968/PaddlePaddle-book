

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>DeepSpeech2 on PaddlePaddle: Design Doc &mdash; PaddlePaddle  文档</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="索引"
              href="../../genindex.html"/>
        <link rel="search" title="搜索" href="../../search.html"/>
    <link rel="top" title="PaddlePaddle  文档" href="../../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index_cn.html" class="icon icon-home"> PaddlePaddle
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../getstarted/index_cn.html">新手入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../build_and_install/index_cn.html">安装与编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../howto/index_cn.html">进阶使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/index_cn.html">开发标准</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/index_cn.html">FAQ</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index_cn.html">PaddlePaddle</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index_cn.html">Docs</a> &raquo;</li>
      
    <li>DeepSpeech2 on PaddlePaddle: Design Doc</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/design/speech/deep_speech_2.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="deepspeech2-on-paddlepaddle-design-doc">
<span id="deepspeech2-on-paddlepaddle-design-doc"></span><h1>DeepSpeech2 on PaddlePaddle: Design Doc<a class="headerlink" href="#deepspeech2-on-paddlepaddle-design-doc" title="永久链接至标题">¶</a></h1>
<p>We are planning to build Deep Speech 2 (DS2) [<a class="reference external" href="#references">1</a>], a powerful Automatic Speech Recognition (ASR) engine,  on PaddlePaddle. For the first-stage plan, we have the following short-term goals:</p>
<ul class="simple">
<li>Release a basic distributed implementation of DS2 on PaddlePaddle.</li>
<li>Contribute a chapter of Deep Speech to PaddlePaddle Book.</li>
</ul>
<p>Intensive system optimization and low-latency inference library (details in [<a class="reference external" href="#references">1</a>]) are not yet covered in this first-stage plan.</p>
<div class="section" id="table-of-contents">
<span id="table-of-contents"></span><h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="#tasks">Tasks</a></li>
<li><a class="reference external" href="#task-dependency">Task Dependency</a></li>
<li><a class="reference external" href="#design-details">Design Details</a><ul>
<li><a class="reference external" href="#overview">Overview</a></li>
<li><a class="reference external" href="#row-convolution">Row Convolution</a></li>
<li><a class="reference external" href="#beam-search-with-ctc-and-lm">Beam Search With CTC and LM</a></li>
</ul>
</li>
<li><a class="reference external" href="#future-work">Future Work</a></li>
<li><a class="reference external" href="#references">References</a></li>
</ul>
</div>
<div class="section" id="tasks">
<span id="tasks"></span><h2>Tasks<a class="headerlink" href="#tasks" title="永久链接至标题">¶</a></h2>
<p>We roughly break down the project into 14 tasks:</p>
<ol class="simple">
<li>Develop an <strong>audio data provider</strong>:<ul>
<li>Json filelist generator.</li>
<li>Audio file format transformer.</li>
<li>Spectrogram feature extraction, power normalization etc.</li>
<li>Batch data reader with SortaGrad.</li>
<li>Data augmentation (optional).</li>
<li>Prepare (one or more) public English data sets &amp; baseline.</li>
</ul>
</li>
<li>Create a <strong>simplified DS2 model configuration</strong>:<ul>
<li>With only fixed-length (by padding) audio sequences (otherwise need <em>Task 3</em>).</li>
<li>With only bidirectional-GRU (otherwise need <em>Task 4</em>).</li>
<li>With only greedy decoder (otherwise need <em>Task 5, 6</em>).</li>
</ul>
</li>
<li>Develop to support <strong>variable-shaped</strong> dense-vector (image) batches of input data.<ul>
<li>Update <code class="docutils literal"><span class="pre">DenseScanner</span></code> in <code class="docutils literal"><span class="pre">dataprovider_converter.py</span></code>, etc.</li>
</ul>
</li>
<li>Develop a new <strong>lookahead-row-convolution layer</strong> (See [<a class="reference external" href="#references">1</a>] for details):<ul>
<li>Lookahead convolution windows.</li>
<li>Within-row convolution, without kernels shared across rows.</li>
</ul>
</li>
<li>Build KenLM <strong>language model</strong> (5-gram) for beam search decoder:<ul>
<li>Use KenLM toolkit.</li>
<li>Prepare the corpus &amp; train the model.</li>
<li>Create infererence interfaces (for Task 6).</li>
</ul>
</li>
<li>Develop a <strong>beam search decoder</strong> with CTC + LM + WORDCOUNT:<ul>
<li>Beam search with CTC.</li>
<li>Beam search with external custom scorer (e.g. LM).</li>
<li>Try to design a more general beam search interface.</li>
</ul>
</li>
<li>Develop a <strong>Word Error Rate evaluator</strong>:<ul>
<li>update <code class="docutils literal"><span class="pre">ctc_error_evaluator</span></code>(CER) to support WER.</li>
</ul>
</li>
<li>Prepare internal dataset for Mandarin (optional):<ul>
<li>Dataset, baseline, evaluation details.</li>
<li>Particular data preprocessing for Mandarin.</li>
<li>Might need cooperating with the Speech Department.</li>
</ul>
</li>
<li>Create <strong>standard DS2 model configuration</strong>:<ul>
<li>With variable-length audio sequences (need <em>Task 3</em>).</li>
<li>With unidirectional-GRU + row-convolution (need <em>Task 4</em>).</li>
<li>With CTC-LM beam search decoder (need <em>Task 5, 6</em>).</li>
</ul>
</li>
<li>Make it run perfectly on <strong>clusters</strong>.</li>
<li>Experiments and <strong>benchmarking</strong> (for accuracy, not efficiency):<ul>
<li>With public English dataset.</li>
<li>With internal (Baidu) Mandarin dataset (optional).</li>
</ul>
</li>
<li>Time <strong>profiling</strong> and optimization.</li>
<li>Prepare <strong>docs</strong>.</li>
<li>Prepare PaddlePaddle <strong>Book</strong> chapter with a simplified version.</li>
</ol>
</div>
<div class="section" id="task-dependency">
<span id="task-dependency"></span><h2>Task Dependency<a class="headerlink" href="#task-dependency" title="永久链接至标题">¶</a></h2>
<p>Tasks parallelizable within phases:</p>
<p>Roadmap     | Description                               | Parallelizable Tasks
&#8212;&#8212;&#8212;&#8211; | :&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;     | :&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;
Phase I     | Simplified model &amp; components             | <em>Task 1</em> ~ <em>Task 8</em>
Phase II    | Standard model &amp; benchmarking &amp; profiling | <em>Task 9</em> ~ <em>Task 12</em>
Phase III   | Documentations                            | <em>Task13</em> ~ <em>Task14</em></p>
<p>Issue for each task will be created later. Contributions, discussions and comments are all highly appreciated and welcomed!</p>
</div>
<div class="section" id="design-details">
<span id="design-details"></span><h2>Design Details<a class="headerlink" href="#design-details" title="永久链接至标题">¶</a></h2>
<div class="section" id="overview">
<span id="overview"></span><h3>Overview<a class="headerlink" href="#overview" title="永久链接至标题">¶</a></h3>
<p>Traditional <strong>ASR</strong> (Automatic Speech Recognition) pipelines require great human efforts devoted to elaborately tuning multiple hand-engineered components (e.g. audio feature design, accoustic model, pronuncation model and language model etc.). <strong>Deep Speech 2</strong> (<strong>DS2</strong>) [<a class="reference external" href="#references">1</a>], however, trains such ASR models in an end-to-end manner, replacing most intermediate modules with only a single deep network architecture. With scaling up both the data and model sizes, DS2 achieves a very significant performance boost.</p>
<p>Please read Deep Speech 2 [<a class="reference external" href="#references">1</a>,<a class="reference external" href="#references">2</a>] paper for more background knowledge.</p>
<p>The classical DS2 network contains 15 layers (from bottom to top):</p>
<ul class="simple">
<li><strong>Two</strong> data layers (audio spectrogram, transcription text)</li>
<li><strong>Three</strong> 2D convolution layers</li>
<li><strong>Seven</strong> uni-directional simple-RNN layers</li>
<li><strong>One</strong> lookahead row convolution layers</li>
<li><strong>One</strong> fully-connected layers</li>
<li><strong>One</strong> CTC-loss layer</li>
</ul>
<div align="center">
<img src="image/ds2_network.png" width=350><br/>
Figure 1. Archetecture of Deep Speech 2 Network.
</div><p>We don&#8217;t have to persist on this 2-3-7-1-1-1 depth [<a class="reference external" href="#references">2</a>]. Similar networks with different depths might also work well. As in [<a class="reference external" href="#references">1</a>], authors use a different depth (e.g. 2-2-3-1-1-1) for final experiments.</p>
<p>Key ingredients about the layers:</p>
<ul class="simple">
<li><strong>Data Layers</strong>:<ul>
<li>Frame sequences data of audio <strong>spectrogram</strong> (with FFT).</li>
<li>Token sequences data of <strong>transcription</strong> text (labels).</li>
<li>These two type of sequences do not have the same lengthes, thus a CTC-loss layer is required.</li>
</ul>
</li>
<li><strong>2D Convolution Layers</strong>:<ul>
<li>Not only temporal convolution, but also <strong>frequency convolution</strong>. Like a 2D image convolution, but with a variable dimension (i.e. temporal dimension).</li>
<li>With striding for only the first convlution layer.</li>
<li>No pooling for all convolution layers.</li>
</ul>
</li>
<li><strong>Uni-directional RNNs</strong><ul>
<li>Uni-directional + row convolution: for low-latency inference.</li>
<li>Bi-direcitional + without row convolution: if we don&#8217;t care about the inference latency.</li>
</ul>
</li>
<li><strong>Row convolution</strong>:<ul>
<li>For looking only a few steps ahead into the feature, instead of looking into a whole sequence in bi-directional RNNs.</li>
<li>Not nessesary if with bi-direcitional RNNs.</li>
<li>&#8220;<strong>Row</strong>&#8221; means convolutions are done within each frequency dimension (row), and no convolution kernels shared across.</li>
</ul>
</li>
<li><strong>Batch Normalization Layers</strong>:<ul>
<li>Added to all above layers (except for data and loss layer).</li>
<li>Sequence-wise normalization for RNNs: BatchNorm only performed on input-state projection and not state-state projection, for efficiency consideration.</li>
</ul>
</li>
</ul>
<p>Required Components                     | PaddlePaddle Support                      | Need to Develop
:&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-  | :&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;   | :&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;
Data Layer I (Spectrogram)              | Not supported yet.                        |  TBD (Task 3)
Data Layer II (Transcription)           | <code class="docutils literal"><span class="pre">paddle.data_type.integer_value_sequence</span></code> | -
2D Convolution Layer                    | <code class="docutils literal"><span class="pre">paddle.layer.image_conv_layer</span></code>           | -
DataType Converter (vec2seq)            | <code class="docutils literal"><span class="pre">paddle.layer.block_expand</span></code>               | -
Bi-/Uni-directional RNNs                | <code class="docutils literal"><span class="pre">paddle.layer.recurrent_group</span></code>            | -
Row Convolution Layer                   | Not supported yet.                        | TBD (Task 4)
CTC-loss Layer                          | <code class="docutils literal"><span class="pre">paddle.layer.warp_ctc</span></code>                   | -
Batch Normalization Layer               | <code class="docutils literal"><span class="pre">paddle.layer.batch_norm</span></code>                 | -
CTC-Beam search                         | Not supported yet.                        | TBD (Task 6)</p>
</div>
<div class="section" id="row-convolution">
<span id="row-convolution"></span><h3>Row Convolution<a class="headerlink" href="#row-convolution" title="永久链接至标题">¶</a></h3>
<p>TODO by Assignees</p>
</div>
<div class="section" id="beam-search-with-ctc-and-lm">
<span id="beam-search-with-ctc-and-lm"></span><h3>Beam Search with CTC and LM<a class="headerlink" href="#beam-search-with-ctc-and-lm" title="永久链接至标题">¶</a></h3>
<div align="center">
<img src="image/beam_search.png" width=600><br/>
Figure 2. Algorithm for CTC Beam Search Decoder.
</div><ul class="simple">
<li>The <strong>Beam Search Decoder</strong> for DS2 CTC-trained network follows the similar approach in [<a class="reference external" href="#references">3</a>] as shown in Figure 2, with two important modifications for the ambiguous parts:<ul>
<li><ol class="first">
<li>in the iterative computation of probabilities, the assignment operation is changed to accumulation for one prefix may comes from different paths;</li>
</ol>
</li>
<li><ol class="first">
<li>the if condition <code class="docutils literal"><span class="pre">if</span> <span class="pre">l^+</span> <span class="pre">not</span> <span class="pre">in</span> <span class="pre">A_prev</span> <span class="pre">then</span></code> after probabilities&#8217; computation is deprecated for it is hard to understand and seems unnecessary.</li>
</ol>
</li>
</ul>
</li>
<li>An <strong>external scorer</strong> would be passed into the decoder to evaluate a candidate prefix during decoding whenever a white space appended in English decoding and any character appended in Mandarin decoding.</li>
<li>Such external scorer consists of language model, word count or any other custom scorers.</li>
<li>The <strong>language model</strong> is built from Task 5, with parameters should be carefully tuned to achieve minimum WER/CER (c.f. Task 7)</li>
<li>This decoder needs to perform with <strong>high efficiency</strong> for the convenience of parameters tuning and speech recognition in reality.</li>
</ul>
</div>
</div>
<div class="section" id="future-work">
<span id="future-work"></span><h2>Future Work<a class="headerlink" href="#future-work" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li>Efficiency Improvement</li>
<li>Accuracy Improvement</li>
<li>Low-latency Inference Library</li>
<li>Large-scale benchmarking</li>
</ul>
</div>
<div class="section" id="references">
<span id="references"></span><h2>References<a class="headerlink" href="#references" title="永久链接至标题">¶</a></h2>
<ol class="simple">
<li>Dario Amodei, etc., <a class="reference external" href="http://proceedings.mlr.press/v48/amodei16.pdf">Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</a>. ICML 2016.</li>
<li>Dario Amodei, etc., <a class="reference external" href="https://arxiv.org/abs/1512.02595">Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</a>.   arXiv:1512.02595.</li>
<li>Awni Y. Hannun, etc. <a class="reference external" href="https://arxiv.org/abs/1408.2873">First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs</a>. arXiv:1408.2873</li>
</ol>
</div>
</div>


           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, PaddlePaddle developers.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>